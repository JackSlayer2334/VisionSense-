ğŸ”® VisionSense+
AI-Powered Multimodal Assistance for the Visually Impaired

Author: Ayushman Yadav

<p align="center"> <img src="https://img.shields.io/badge/Author-Ayushman%20Yadav-blue?style=for-the-badge"> <img src="https://img.shields.io/badge/AI-Computer%20Vision-purple?style=for-the-badge"> <img src="https://img.shields.io/badge/LLM-Scene%20Understanding-green?style=for-the-badge"> <img src="https://img.shields.io/badge/Model-YOLOv8-orange?style=for-the-badge"> <img src="https://img.shields.io/badge/API-FastAPI-yellow?style=for-the-badge"> </p>
ğŸ§  Overview

VisionSense+ is an intelligent multimodal AI system designed to help visually impaired users understand their surroundings in real-time. It combines:

YOLOv8 ONNX for object detection

EasyOCR for reading text in the environment

GPT-powered LLM for natural-language scene explanation

TTS (Text-to-Speech) for audio feedback

This project demonstrates end-to-end AI engineering, making it ideal for:

ML Engineer internships

AI/Computer Vision roles

Full-stack ML system building

College major/minor projects

âœ¨ Features
ğŸ” Object Detection (real-time)

Using YOLOv8 (converted to ONNX), VisionSense+ can detect:

People

Cars, bikes, traffic lights

Animals

Chairs, furniture

Obstacles & more (80 COCO classes)

ğŸ“ OCR (EasyOCR)

Reads environmental text:

Signboards

Navigation boards

Shop names

Instructions

Documents

ğŸ§  Scene Analysis (LLM)

An LLM combines detected objects + text and generates a helpful, safe, and human-like explanation.

Example output:

â€œA person is standing 2 meters ahead. A car is approaching from the right. The signboard reads â€˜Metro Station Gate Aâ€™.â€

ğŸ”Š Audio Output (TTS)

Scene explanation is spoken aloud for blind users.

ğŸ—ï¸ Project Structure
VisionSense-/
â”‚â”€â”€ api/
â”‚ â”œâ”€â”€ main.py # FastAPI app
â”‚ â”œâ”€â”€ detection.py # YOLOv8 ONNX inference
â”‚ â”œâ”€â”€ ocr.py # Text detection
â”‚ â”œâ”€â”€ llm.py # Scene explanation (GPT or fallback)
â”‚ â”œâ”€â”€ tts.py # Text-to-Speech engine
â”‚ â”œâ”€â”€ utils.py # Preprocessing, NMS, scaling
â”‚ â””â”€â”€ config.py # Settings
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ yolov8.onnx # (ignored in git)
â”‚
â”œâ”€â”€ samples/ # Test images
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ ...

âš™ï¸ Installation
1ï¸âƒ£ Clone the repo
git clone https://github.com/JackSlayer2334/VisionSense-.git
cd VisionSense-

2ï¸âƒ£ Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Download YOLO model
pip install ultralytics
yolo predict model=yolov8n.pt source=None
cp ~/.config/Ultralytics/yolov8n.pt models/

5ï¸âƒ£ Export to ONNX (compatible opset)
yolo export model=models/yolov8n.pt format=onnx opset=12 imgsz=640
mv yolov8n.onnx models/yolov8.onnx

ğŸš€ Run the Server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

Visit API interface:

http://localhost:8000/docs

ğŸ§ª Test With an Image
curl -X POST "http://localhost:8000/analyze" \
 -F "file=@samples/test.jpg"

ğŸ“¸ Screenshots / Demo (Add later)

Object Detection Output

OCR Output

Scene Explanation Output

API documentation screenshot

ğŸ§© Roadmap

Real-time video streaming

Mobile app (React Native)

Raspberry Pi support

Offline small-LLM mode

Cloud deployment (Railway/Render/AWS)

ğŸ§‘â€ğŸ’» Skills Demonstrated

This project showcases:

Computer Vision (YOLOv8, ONNX Runtime)

NLP + LLM integration

FastAPI backend engineering

Real-time inference pipeline

Modular ML system design

Git & best practices

Clean architecture and scalability

ğŸ‘¨â€ğŸ’» Author

Ayushman Yadav
AI/ML Developer | Computer Vision | Backend | Data Structures

ğŸ“ License

This project is released under the MIT License.
