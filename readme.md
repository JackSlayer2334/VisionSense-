<h1 align="center">ğŸ”® VisionSense+ Real-Time Multimodal AI Assistance for the Visually Impaired</h1> <p align="center"> <img src="https://img.shields.io/badge/AI-Computer Vision-blue?style=for-the-badge"> <img src="https://img.shields.io/badge/LLM-Scene Understanding-purple?style=for-the-badge"> <img src="https://img.shields.io/badge/Tech-FastAPI-green?style=for-the-badge"> <img src="https://img.shields.io/badge/Model-YOLOv8-orange?style=for-the-badge"> </p>
ğŸš€ Overview

VisionSense+ is a real-time AI system designed to assist visually impaired users using:

ğŸ§  Multimodal AI

YOLOv8 ONNX for object detection

EasyOCR for reading text

LLM (OpenAI or fallback offline) for scene explanation

TTS (Text-to-Speech) for audio guidance

The goal is simple:

Help blind users understand their surroundings through AI-powered audio descriptions.

This is a production-ready ML engineering portfolio project demonstrating:

Computer Vision

LLM integration

Real-time inference

API engineering

End-to-end AI system design

âœ¨ Features
ğŸ” Real-Time Object Detection

Detects:

People, cars, bikes

Stairs, chairs, obstacles

Traffic lights

Animals & more

ğŸ“ OCR Text Reading

Reads:

Signs

Bus numbers

Menus

Documents

ğŸ“¢ Audio Scene Description

AI generates:

A short natural scene summary

Safety warning

Object overview

ğŸŒ FastAPI Backend

Clean, modular API:

/analyze â†’ upload image â†’ returns detections + text + AI description

ğŸ§ Optional TTS Support

Direct audio feedback for blind users.

ğŸ—ï¸ Project Structure
vision-sense/
â”‚â”€â”€ api/
â”‚ â”œâ”€â”€ main.py # FastAPI app
â”‚ â”œâ”€â”€ detection.py # YOLOv8 ONNX inference
â”‚ â”œâ”€â”€ ocr.py # EasyOCR wrapper
â”‚ â”œâ”€â”€ llm.py # LLM scene explanation
â”‚ â”œâ”€â”€ tts.py # Text-to-speech
â”‚ â”œâ”€â”€ utils.py # Preprocessing & NMS
â”‚ â””â”€â”€ config.py # Settings
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ yolov8.onnx # (required) ONNX model file
â”‚
â”œâ”€â”€ samples/ # Sample images to test
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ ...

âš™ï¸ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/<your-username>/vision-sense.git
cd vision-sense

2ï¸âƒ£ Create a virtual environment
python3 -m venv .venv
source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Get YOLOv8 ONNX model
pip install ultralytics
yolo export model=yolov8n.pt format=onnx imgsz=640

Move the file:

yolov8n.onnx â†’ models/yolov8.onnx

ğŸƒ Run the Server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

Open docs:

http://localhost:8000/docs

ğŸ§ª Test the API

Place image:

samples/test.jpg

Then test:

curl -X POST "http://localhost:8000/analyze" \
 -F "file=@samples/test.jpg"

Example Output:

{
"detections": {
"boxes": [...],
"scores": [...],
"labels": ["person", "car"]
},
"text": ["Metro Station Entrance"],
"description": "A person is near the entrance. A car is approaching from the right."
}

ğŸ“¸ Screenshots (add when ready)
[ ] object detection output  
[ ] OCR output  
[ ] FastAPI docs screenshot

ğŸ§  How It Works (Architecture)
Camera â†’ Preprocessing â†’ YOLOv8 ONNX â†’ OCR â†’ LLM â†’ Audio/TTS â†’ Blind User

YOLO detects objects

OCR extracts readable text

LLM combines everything into a description

TTS speaks it aloud

ğŸ“¦ Roadmap

Real-time video streaming

Edge-device support (Raspberry Pi)

Offline small LLM (Llama 3.1 3B)

React Native mobile app

Navigation assistance (GPS-based)

ğŸ§‘â€ğŸ’» Skills Demonstrated

This project showcases:

Computer Vision (ONNX Runtime, preprocessing)

Multimodal ML pipelines

Real-time inference optimization

FastAPI backend design

LLM prompt engineering

API architecture

TTS integration

Model deployment workflow

Perfect for:

ML Engineer

Computer Vision Engineer

AI/ML Intern

AI Research Assistant

ğŸ“ License

MIT License

â¤ï¸ Acknowledgements

Ultralytics YOLO

EasyOCR

OpenAI GPT Models

FastAPI
